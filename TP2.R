###########################################
# TP 2 - J√©r√¥me CR√âTIEN - Florent RENUCCI #
# 16 octobre - pour le 20 novembre		  #
###########################################


#################
# Application 1 #
#################
rm(list=ls())

file = '/Users/JEROME/Documents/My-Docs/Etudes/ECP/3A/OMA/OMA - 2012-2013/Statistiques - Data_Mining - Apprentissage/TP/TP-2/UsCrime.txt'
file = "D:/frenucci/Dropbox/DonnÈes/Scolaire/cours/3A/Statistiques et Data-Mining/TP/TP2/UsCrime.txt"
tab = read.table(file,sep='\t',header=TRUE)

# Nombre d'observations disponibles :
nrow(tab)
# On a 47 observations

pairs(tab,pch=22,bg=c('red','blue')[unclass(factor(tab[,'R']))])
# On remarqe √† l'oeil nu que certaines variables sont fortement corr√©l√©es. 
# Par exemple U1 et U2, qui repr√©sentent le taux de ch√¥mage pour les 14-24 ans et les 35-39 ans respectivement. Il est √©vident que ces 2 variables sont corr√©l√©es (sans pour autant √™tre li√©es par une relation de causalit√© directe) : si le taux de ch√¥mage de l'Etat augmente, il augmente pour les 2 tranches d'√¢ge.
# Autre exemple, W et X sont aussi corr√©l√©es (n√©gativement cette fois) : apparemment plus la m√©diane de la richesse d'une famille est grande (donc plus les familles de l'Etat sont riches), moins nombreuses seront les familles dont la richesse se situe sous la moiti√© de la m√©diane (donc moins il y aura de "tr√®s pauvres"). 
# Autre exemple, Ex0 et Ex1 sont aussi corr√©l√©es positivement : plus les d√©penses pour la police sont importantes en 1959, plus elles le seront en 1960, pour un √©tat donn√©.

cov(tab)	# matrice de covariance
cor(tab)	# matrice de corr√©lation

# Comme intuit√© pr√©c√©demment, Ex0 et Ex1 sont fortement corr√©l√©s (0.99), U1 et U2 aussi (0.74), et W et X sont corr√©l√©s n√©gativement (-0.88).
# D'une mani√®re g√©n√©rale, un r√©sultat proche de -1 explique une forte corr√©lation n√©gative (l'augmentation d'une variable entraine la diminution de l'autre, presque lin√©airement), un r√©sultat proche de 1 explique √† l'inverse une forte corr√©lation positive. Un r√©sultat proche de 0 d√©note d'une certaine ind√©pendance.

#################################
# Mod√®le de r√©gression lin√©aire #
#################################

# le mod√®le est de type Y = XB + epsilon. Y la variable expliqu√©e, X les variables explicatives (matrice n*K, n observations, K variables), B un vecteur de taille K, epsilon une perturbation (vecteur de taille n). On cherche les coefficients de B pour ensuite pouvoir pr√©dire Y.

help(lm)
res = lm('R~.',data=tab)


# Question 1
#-----------
print(res)
summary(res)
attributes(res)

# plus la t-value (statistique de Student) est faible en valeur absolue (ce qui correspond √† une forte p-value), moins la variable est significative. A un risque fix√©, elle permet donc d'accepter ou de rejeter H0. Avec ici H0 = "coefficient de corr√©lation = 0" (ie "le coefficient n'est pas significatif").
# Ed, X, Age et U2 sont significatifs ici. L'intercept l'est √©galement : Y n'est donc pas centr√©e.
# les coefficients estim√©s se lisent dans la premi√®re colonne (Age = 1.040, S = -8.308 etc...).

# Question 2
#-----------

# R^2 = SCE/SCT avec SCE = variance estim√©e et SCT = variance r√©elle.

# On lit dans summary(res) :
# R^2 = 0.7692
# C'est aussi : R^2 :=
var(res$fit)/var(tab$R)
# R^2 relativement proche de 1, donc mod√®le globalement significatif
# La significativit√© globale du mod√®le est en fait donn√©e par la statistique de Fisher => summary(fit) donne sa valeur et sa p-value : 3.686e-07 qui est tr√®s faible. Il y a donc au moins un coefficient non nul (le mod√®le est globalement significatif).

# Question 3
#-----------
# summary(res) donne toutes les informations n√©cessaires √† juger la significativit√© de chacun des coefficients.
# L'hypoth√®se nulle test√©e est la non significativit√© de chaque coefficient (H0 = "coefficient de corr√©lation = 0"). Plus la p-value est faible, plus H0 doit √™tre rejet√©e.
# summary(res) retranscrit cette information par les codes ., *, **, *** qui expriment le niveau de confiance (ou de risque) attach√© √† la significativit√© de chaque coefficient, selon la l√©gende suivante : 0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1
# par exemple, un coefficient *** est significatif, au risque 1 pour 1000 (ou moins).
# Comme dit plus haut : Ed, X, Age et U2 sont significatifs ici. L'intercept l'est √©galement : Y n'est donc pas centr√©e.


# Intervalles de confiance :
#---------------------------
confint(res,level=0.95)	# intervalle de confiance au risque 5%
confint(res,level=0.99)	# intervalle de confiance au risque 1%

# On remarque que les intervalles de confiance augmentent quand le niveau de risque diminue (ce qui est attendu) et qu'ils sont centr√©s sur la valeur estim√©e (√©galement attendu).
# Pour d√©cider de la significativit√© d'un coefficient, on peut v√©rifier si la valeur 0 appartient ou non √† l'intervalle de confiance. Si c'est le cas, on consi√®re que le coefficient n'est pas significatif (qu'il est nul).
# On retrouve bien que Ed, X et l'intercept sont significatifs au risque 1% et que Age et U2 le sont au risque 5%.


# Question 4
#-----------
# REM : res$fit = predict(res)
plot(tab$R,res$fit)
# On est autour de la 1√®re bissectrice, ce qui tend √† montrer que la r√©gression permet d'expliquer la variable √©tudi√©e.

help(predict.lm)
predict(res,tab,interval='confidence',level=0.95)
predict(res,tab,interval='prediction',level=0.95)
print(cbind(predict(res,tab,interval='confidence',level=0.95),tab$R))
# les observtions 2 et 3 ne sont pas dans l'intervalle de confiance...
print(cbind(predict(res,tab,interval='prediction',level=0.95),tab$R))
# l'observtion 11 n'est pas dans l'intervalle de prediction. Pourtant, comme cet intervalle prend en compte la perturbation, il est plus grand que l'intervalle de confiance.
# Ceci montre les limites pr√©dictives de notre mod√®le (tout en nous rassurant quant √† un probl√®me de surapprentissage).

# Question 5
#-----------
# Erreur quadratique des r√©sidus : 
sum(res$residuals^2)
# un r√©sidu sur une observation est (mesure - fit) ^ 2. S'ils sont tr√®s faibles, le risque est d'overfitter (trop "coller" aux donn√©es, et trouver un mod√®le tr√®s complexe, en oubliant donc la r√©alit√© de la question), s'ils sont tr√®s forts, le risque est d'oversmoother (trop "survoler" les donn√©es et donc ce qu'elles repr√©sentent, pour favoriser un mod√®le plus simple).

# Variance r√©siduelle :
var(res$residuals)
# On a variance totale = variance r√©siduelle + variance expliqu√©e. On veut donc que variance r√©siduelle/variance totale soit proche de 0.
var(res$residuals)/(var(res$fit)+var(res$residuals))
# on trouve 23%, donc on explique 77% de la variance gr√¢ce au fit, ce qui est satisfaisant.

plot(tab$R,res$residuals)
# On cherche ici √† √©valuer l'homo/h√©t√©ro-sc√©dasticit√© des r√©sidus. Sur le graphe, on n'observe pas de variabilit√© particuli√®re des r√©sidus. On peut donc raisonablement penser qu'ils sont bien homo-sc√©sastique, ce qui valide (sous r√©serve de gaussianit√©) la d√©marche du mod√®le lin√©aire.

# On trace le qqplot des r√©sidus :
qqnorm(res$residuals)
# Il nous para√Æt plus lisible de renormaliser :
qqnorm(res$residuals/sqrt(var(res$residuals)))
# Les points sont assez bien align√©s sur la 1√®re bissectrice :
# masse centrale des r√©sidus bien align√©s sur les quantiles de la loi normale
# qq outliers pour les valeurs extr√™mes
# On peut ainsi conclure graphiquement √† la gaussianit√© des r√©sidus.

shapiro.test(res$residuals)
# Grande p-value devant 0.05 => pas de raison de rejeter l'hypoth√®se gaussienne.
# Cela confirme le r√©sultat pr√©c√©dent et valide donc la d√©marche du mod√®le lin√©aire : r√©sidus gaussiens et homosc√©dastiques.


# Question 6
#-----------
indTest = seq(1,nrow(tab),3)
tabTest = tab[indTest,]
tabTrain = tab[-indTest,]
resTrain = lm('R~.',data=tabTrain)
predTest = predict.lm(resTrain,tabTest)
resTest = predTest - tabTest$R
# L'erreur quadratique moyenne n'est pas tr√®s satisfaisante :
sqrt(var(resTest))

# On peut √©galement calculer la part de variance r√©siduelle :
var(resTest)/(var(predTest)+var(resTest))
# on trouve 51%, donc on explique 49% de la variance sur la base de test gr√¢ce au mod√®le. C'est inf√©rieur au r√©sultat pr√©c√©dent, certainement parce que la base d'apprentissage est moins grande (2/3 de la pr√©c√©dente).

# Question 7
#-----------
x11();par(mfrow=c(2,2));plot(res);
# Les 2 graphes du haut ont d√©j√† √©t√© trac√©s √† la question 5 :
# Le 1er permet de visualiser graphiquement la variance (homo/h√©t√©ro-sc√©dastique) - ici, on conclut √† l'homosc√©dasticit√©
# Le 2√®me permet de voir si les donn√©es sont gaussiennes ou non : QQ-plot.
# Les 2 graphes du bas permettent de tester l'homosc√©dasticit√© de mani√®re plus pr√©cise ('effet levier'... pour trouver des outliers...)

########################
# S√©lection de mod√®les #
########################

# 1 - R√©gression Backward
#------------------------
regbackward = step(res,direction='backward')
summary(regbackward)
# On enl√®ve la variable qui a le plus grand RSS (crit√®re AIC => mais √ßa revient √† consid√©rer le RSS ie les r√©sidus)
# A chaque √©tape, on calcule les RSS associ√©s √† chaque variable, on √©liminele plus faible (√† la 1√®re it√©ration par exemple, c'est la variable NW), et on recommence. L'AIC calcul√© √† chaque √©tape repr√©sente la perte en informations lorsqu'on n√©glige une variable. On cherche √† le minimiser, donc lorsque la suppression d'une variable n'entra√Æne plus une baisse de l'AIC, on arr√™te l'algorithme. 
# Finalement, il ne reste que les variables Age, Ed, Ex0, U2, W, X, avec une p-value de 1.441 10^-10. Dans la 1√®re partie on avait une p-value de 3.7 10^-7. 
# Par ailleurs on avait R^2=0.68, on a maintenant 0.71.
# Le mod√®le est plus parcimonieux mais a une plus grande valeur explicative. Les variables √©limin√©es n'apportaient donc pas d'information pertinente.

# 2 - R√©gression Forward
#-----------------------
regforward = step(lm(R~1,data=tab),list(upper=res),direction='forward')
summary(regforward)
# On cherche √† rajouter la variable qui donne le mod√®le avec les plus petits r√©sidus.
# On s'arr√™te quand la p-value du coeff qu'on veut ajouter est inf√©rieure √† un seuil (quand le coeff n'est pas significatif).
# On obtient le m√™me mod√®le parcimonieux avec les 2 m√©thodes, ce qui n'avait rien d'√©vident a priori, puisqu'il s'agit d'optimisation partielle.

# 3 - R√©gression Stepwise
#------------------------
regboth = step(res,direction='both')
summary(regboth)
# Ici, on part du mod√®le nul, qu'on augmente, comme en r√©gression forward. A chaque √©tape, on ajoute la variable qui augmente le plus le RSS, tout en rejetant les variables jug√©es non pertinentes (non significatives pour le mod√®le courant). On s'arr√™te lorsque toutes les variables ont √©t√© s√©lectionn√©es ou rejet√©es.
# Cette approche revient √† combiner les deux pr√©c√©dentes, en regardant les effets de l'ajout ou de la suppression d'une variable √† chaque √©tape.
# Ici encore, on obtient le m√™me mod√®le parcimonieux qu'avec les 2 m√©thodes pr√©c√©dentes (ce qui n'√©tait toujours pas attendu a priori).

# 4 -
#----
# En entrant formula(regboth), on obtient les variables conserv√©es par la r√©gression stepwise :
formula(regboth)
reg0 = lm(formula(regboth),data=tab)

# On retrouve le mod√®le parcimonieux :
summary(reg0)
# On retrouve le mod√®le correspondant au r√©sultat de l'algorithme stepwise, donc avec seulement 6 variables. R^2 et la p-value sont satisfaisants.


####################
# R√©gression LASSO #
####################

# Question 1
#-----------
library(lars)
help(lars)
# la r√©gression Lasso minimise le RSS avec la contrainte : somme des valeurs absolues des coeff < constante (param√®tre √† fixer par le statisticien).
# elle peut √©galement se formaliser comme la minimisation du RSS p√©nalis√© de lambda * la somme des valeurs absolues des coeff (norme 1 de B). Ici, c'est lambda le param√®tre.

# Question 2
#-----------
X = data.matrix(tab[-c(1)])
Y = tab$R
reslasso = lars(X,Y,type='lasso')
plot(reslasso)
# On repr√©sente les coefficients (standardis√©s) en fonction de lambda : c'est le "chemin de r√©gularisation".
# On voit bien ici que le Lasso favorise la parcimonie et la s√©lection de variables : leur nombre diminue quand lambda -> 0.
plot(reslasso$lambda)
# On repr√©sente les lambdas selon la norme 1 de B divis√©e par la norme 1 max obtenue.

# Question 3
#-----------
coef_lasso_l0 = predict.lars(reslasso,X,type='coefficients',mode='lambda',s=0)
# On trace les coefficients :
x11();barplot(coef_lasso_l0$coefficients)
# On peut comparer les r√©sultat du Lasso avec lambda = 0 :
coef_lasso_l0$coef
# Avec ceux du mod√®le lin√©aire trouv√©s plus t√¥t :
res$coef
# On obtient bien la m√™me chose (on fait la m√™me chose, puisque lambda = 0 : on ne p√©nalise pas).
# Seule diff√©rence : l'intercept n'appara√Æt pas dans les coefs du Lasso.

# Question 4
#-----------
coef_lasso_l1 = predict.lars(reslasso,X,type='coefficients',mode='lambda',s=1)
coef_lasso_l4 = predict.lars(reslasso,X,type='coefficients',mode='lambda',s=4)
coef_lasso_l20 = predict.lars(reslasso,X,type='coefficients',mode='lambda',s=20)
# Coefficients LASSO pour lambda = 1 :
coef_lasso_l1$coef
# On a tu√© 3 coefficients
# Coefficients LASSO pour lambda = 4 :
coef_lasso_l4$coef
# On a tu√© 1 coefficient de plus (depuis lambda = 1) [en tout 4]
# Coefficients LASSO pour lambda = 20 :
coef_lasso_l20$coef
# On a tu√© 3 coefficients de plus (depuis lambda = 4) [en tout 7]

# Lorsque lambda augmente, on risque moins d'overfitter, parce que la contrainte de minimisation de B est pr√©pond√®rante.
# Et ce jusqu'√† ce que B arrive √† la norme 0.
# Il faut donc trouver un √©quilibre entre la r√©gression lin√©aire (lambda = 0) et une annulation de B.
# Ceci revient √† la probl√©matique d'optimisation biais/variance.

# La capacit√© de s√©lection de variable du Lasso appara√Æt encore ici.


# Question 5
#-----------
pY = predict.lars(reslasso,X,type='fit',model='lambda',s=2)
sqrt(var(Y - pY$fit))
# l'erreur quadratique moyenne est plus √©lev√©e que dans le mod√®le lin√©aire.

# On peut √©galement consid√©rer :
var(Y-pY$fit)/var(Y)
# Var totale = var expliqu√©e + var r√©siduelle
# ici on explique donc 44% de la variance. Le mod√®le lin√©aire donnait de meilleurs r√©sultats.
# Ainsi, la s√©lection de variables, qui rend le mod√®le plus parcimonieux et plus robuste, lui a h√¥t√© de son caract√®re pr√©dictif.


####################
# R√©gression RIDGE #
####################

# Question 1
#-----------
# la r√©gression Ridge minimise le RSS avec la contrainte : norme 2 du vecteur coeff < constante (param√®tre √† fixer par le statisticien).
# elle peut √©galement se formaliser comme la minimisation du RSS p√©nalis√© de lambda * la norme 2 de B. Ici, c'est lambda le param√®tre.
# enfin, l'estimateur Ridge s'exprime explicitement en fonction de lambda, X et Y.
library(MASS)
help(lm.ridge)

# Question 2
#-----------
resridge_l0 = lm.ridge('R~.',data=tab,lambda=0)
resridge_l100 = lm.ridge('R~.',data=tab,lambda=100)

resridge_l0$coef	# ne retourne pas l'intercep
coef(resridge_l0)	# retourne tous les coefs, yc l'intercept
resridge_l0			# idem coef(resridge_l0)

resridge_l0
# On obtient les m√™mes r√©sultats qu'avec le mod√®le lin√©aire (puisqu'on ne le p√©nalise pas ici).
resridge_l100
# Ici, la p√©nalit√© est importante. Beaucoup de coefficients (les moins significatifs) voient leur valeur absolue diminuer de beaucoup. Neanmoins, contrairement au Lasso, Ridge ne s√©lectionne pas les variables (du moins pas explicitement : il faudrait fixer un seuil de significativit√©...).	

# Question 3
#-----------
lambda = seq(0,100,0.01)
resridge = lm.ridge('R~.',data=tab,lambda=lambda)
plot(lambda,resridge$GCV,type='l')
plot(resridge)	# chemin de r√©gularisation
# Certains coeffs changent de signe (ce qui peut surprendre a prrmi√®re vue...)
# Tous les coefficients tendent vers 0.
# Encore une fois, la s√©lection de variables est moins efficace qu'avec le Lasso.

# On choisit le mod√®le √† retenir en minimisant GCV. On obtient lambda = 2 environ, par lecture sur plot(lambda,resridge$GCV,type='l').
# Une approche plus pr√©cise donne lambda optimal = 2,19, atteint en 220 :
which.min(resridge$GCV)
# Ainsi, les coefs du mod√®le sont en coef(resridge)[220,] :
Coefridge = coef(resridge)[220,]
Coefridge

# Question 4
#-----------
Xridge=cbind(rep(1,47),tab[,2:14])
Yridge=as.matrix(Xridge)%*%as.vector(Coefridge)
# Ici Yridge = X.B. On trouve Yridge environ √©gal √† Y.

# L'erreur quadratique moyenne est :
sqrt(var(Y-Yridge))
# C'est la plus faible qu'on ait obtenue !

# On calcule la part de variance r√©siduelle :
var(Y-Yridge)/var(Y)
# Ici, on explique 75% de la variance. C'est le meilleur r√©sultat.

# Il semble donc que la r√©gression Ridge ait le meilleur pouvoir pr√©dictif pour le jeu de donn√©es consid√©r√©.
# Notons malgr√© tout qu'il s'agit de la solution optimale en lambda (ce qui n'√©tait pas le cas a priori pour le Lasso).

##########################################
# Comparaison des m√©thodes de r√©gression #
##########################################

# Question 1
#-----------
learning_indices = sample(nrow(tab),0.70*nrow(tab))
# on choisit 70% des lignes, au hasard, qui formeront le learning set
learning_set = tab[learning_indices,]
test_set = tab[-learning_indices,]
# le reste des lignes forment la base de test

# Question 2
#-----------
#Mod√®le lin√©aire
res_learning_lm = lm('R~.',data=learning_set)
prediction_test_lm = predict.lm(res_learning_lm,test_set)

sqrt(var(res_learning_lm$res))
sqrt(var(prediction_test_lm-test_set[,1]))

#Backward
regbackward = step(res,direction='backward')
prediction_learning_backward = predict(regbackward,learning_set)
prediction_test_backward = predict(regbackward,test_set)

sqrt(var(prediction_learning_backward-learning_set[,1]))
sqrt(var(prediction_test_backward-test_set[,1]))

#Forward
#-------
regforward = step(res,direction='forward')
prediction_learning_forward = predict(regforward,learning_set)
prediction_test_forward = predict(regforward,test_set)

sqrt(var(prediction_learning_forward-learning_set[,1]))
sqrt(var(prediction_test_forward-test_set[,1]))

#Stepwise
#--------
regstepwise = step(res,direction='both')
prediction_learning_stepwise = predict(regstepwise,learning_set)
prediction_test_stepwise = predict(regstepwise,test_set)

sqrt(var(prediction_learning_stepwise-learning_set[,1]))
sqrt(var(prediction_test_stepwise-test_set[,1]))

#Lasso
#-----
X_learning_set = data.matrix(learning_set[-c(1)])
X_test_set = data.matrix(test_set[-c(1)])

Y_learning_set = learning_set$R
Y_test_set= test_set$R

reglasso = lars(X_learning_set,Y_learning_set,type="lasso")

prediction_learning_lasso = predict.lars(reglasso,X_learning_set,type='fit',mode='lambda',s=4)
prediction_test_lasso = predict.lars(reglasso,X_test_set,type='fit',mode='lambda',s=4)

sqrt(var(prediction_learning_lasso$fit-learning_set[,1]))
sqrt(var(prediction_test_lasso$fit-test_set[,1]))

#Ridge
#-----
X_learning_set_ridge = cbind(rep(1,nrow(learning_set)),learning_set[,2:14])
X_test_set_ridge = cbind(rep(1,nrow(test_set)),test_set[,2:14])

resridge = lm.ridge('R~.',data=learning_set,lambda=lambda)
which.min(resridge$GCV)
# Ainsi, les coefs du mod√®le sont en coef(resridge)[9,] :
Coefridge = coef(resridge)[arrayInd(which.min(resridge$GCV),1000)[1],]


Y_learning_ridge = as.matrix(X_learning_set_ridge)%*%as.vector(Coefridge)
Y_test_ridge = as.matrix(X_test_set_ridge)%*%as.vector(Coefridge)

sqrt(var(Y_learning_ridge-learning_set[,1]))
sqrt(var(Y_test_ridge-test_set[,1]))


# Question 3
#-----------
# apr√®s plusieurs simulations, on remarque que : 
# - le mod√®le lin√©aire fait clairement du surapprentissage, avec une erreur quadratique moyenne faible sur la base d'apprentissage et tr√®s √©lev√©e sur la base de test
# - dans notre cas, backward = stepwise (on le savait d√©j√†) < forward (ces m√©thodes simples semblent performantes ici)
# - le lasso et ridge sont mieux que toutes les autres approches sur la base d'apprentissage, et moins bien sur la base de test (seul le mod√®le lin√©aire fait pire)
#     => peut-√™tre qu'il existe une utilisation de ces m√©thodes dans un certain cadre pour construire un mod√®le parcimonieux sans chercher √† l'utiliser ? 
#     => malgr√© tout, ce constat est surprenant car il devrait √™tre r√©v√©lateur d'un surapprentissage, alors que ridge et lasso sont des m√©thodes parcimonieuses (surtout le lasso) qui visent justement √† construire des mod√®les plus robustes. La selection de donn√©es induite par ces m√©thodes peut-elle cependant conduire √† un surapprentissage ?